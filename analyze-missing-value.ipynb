{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrp = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis Missing Value\n",
    "\n",
    "Terdapat 2 dataset yang tersedia, yakni:\n",
    "1. Data harga saham penutupan untuk perusahaan PT Indofood CBP Sukses Makmur Tbk (ICBP.JK) dari periode 1 Januari 2004 hingga 4 Desember 2007. Data harga saham merupakan data time series dengan total pengamatan sebanyak 1.000 pengamatan.\n",
    "2. Data karakteristik nasabah kartu kredit sebanyak 1.000 orang. Data nasabah merupakan data cross section yang diamati dari individu yang berbeda dalam satu periode yang sama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lakukan random data sebanyak persentase missing dengan seed sesuai 2 digit NRP terakhir. `random.seed()` digunakan agar setiap kali random data akan mendapatkan hasil yang sama. Dengan menggunakan digit seed sesuai NRP, maka observasi yang menjadi missing value antara satu mahasiswa akan berbeda dengan mahasiswa lainnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pengerjaan df_1 (data-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('data/num-1/raw/data-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[562, 324, 136, 571, 729, 548, 310, 813, 961, 518]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing value 1% with seed 48\n",
    "data_obs_1 = list(range(1, 1001, 1))\n",
    "random.seed(nrp)\n",
    "data_obs_1 = random.sample(data_obs_1, 10)\n",
    "\n",
    "data_obs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[562,\n",
       " 324,\n",
       " 136,\n",
       " 571,\n",
       " 729,\n",
       " 548,\n",
       " 310,\n",
       " 813,\n",
       " 961,\n",
       " 518,\n",
       " 198,\n",
       " 732,\n",
       " 781,\n",
       " 447,\n",
       " 170,\n",
       " 159,\n",
       " 990,\n",
       " 827,\n",
       " 895,\n",
       " 674,\n",
       " 111,\n",
       " 505,\n",
       " 665,\n",
       " 226,\n",
       " 224,\n",
       " 524,\n",
       " 570,\n",
       " 871,\n",
       " 521,\n",
       " 484,\n",
       " 223,\n",
       " 29,\n",
       " 76,\n",
       " 434,\n",
       " 66,\n",
       " 511,\n",
       " 689,\n",
       " 167,\n",
       " 974,\n",
       " 132,\n",
       " 58,\n",
       " 715,\n",
       " 299,\n",
       " 959,\n",
       " 348,\n",
       " 716,\n",
       " 346,\n",
       " 98,\n",
       " 862,\n",
       " 245]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing value 5% with seed 48\n",
    "data_obs_5 = list(range(1, 1001, 1))\n",
    "random.seed(nrp)\n",
    "data_obs_5 = random.sample(data_obs_5, 50)\n",
    "\n",
    "data_obs_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[560, 322, 134, 569, 727, 546, 308, 811, 959, 516]\n",
      "[560, 322, 134, 569, 727, 546, 308, 811, 959, 516, 196, 730, 779, 445, 168, 157, 988, 825, 893, 672, 109, 503, 663, 224, 222, 522, 568, 869, 519, 482, 221, 27, 74, 432, 64, 509, 687, 165, 972, 130, 56, 713, 297, 957, 346, 714, 344, 96, 860, 243]\n"
     ]
    }
   ],
   "source": [
    "data_obs_1 = [x - 2 for x in data_obs_1]\n",
    "data_obs_5 = [x - 2 for x in data_obs_5]\n",
    "\n",
    "print(data_obs_1)\n",
    "print(data_obs_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date      0\n",
       "Close    10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_obs_1 as line to replace with NaN in data-1.csv\n",
    "# example: we got [562, 324, 136, 571, 729, 548, 310, 813, 961, 518], so replace line 562, 324, 136, 571, 729, 548, 310, 813, 961, 518 with NaN at column ['Close']\n",
    "# but index start from 2, because line 1 is header\n",
    "# so we got [563, 325, 137, 572, 730, 549, 311, 814, 962, 519]\n",
    "df_1_missing_1 = df_1.copy()\n",
    "# start from 2\n",
    "for i in data_obs_1:\n",
    "    df_1_missing_1.loc[i, 'Close'] = np.nan\n",
    "\n",
    "df_1_missing_1.to_csv('data/num-1/miss/data-1-missing-1.csv', index=False)\n",
    "df_1_missing_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date      0\n",
       "Close    50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1_missing_5 = df_1.copy()\n",
    "\n",
    "for i in data_obs_5:\n",
    "    df_1_missing_5.loc[i, 'Close'] = np.nan\n",
    "\n",
    "df_1_missing_5.to_csv('data/num-1/miss/data-1-missing-5.csv', index=False)\n",
    "\n",
    "# check missing value\n",
    "df_1_missing_5.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pengerjaan df_2 (data-ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('data/num-1/raw/data-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[562, 324, 136, 571, 729, 548, 310, 813, 961, 518]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing value at df_2, 1% with seed 48\n",
    "data_obs_1 = list(range(1, 1001, 1))\n",
    "random.seed(nrp)\n",
    "data_obs_1 = random.sample(data_obs_1, 10)\n",
    "\n",
    "data_obs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[562,\n",
       " 324,\n",
       " 136,\n",
       " 571,\n",
       " 729,\n",
       " 548,\n",
       " 310,\n",
       " 813,\n",
       " 961,\n",
       " 518,\n",
       " 198,\n",
       " 732,\n",
       " 781,\n",
       " 447,\n",
       " 170,\n",
       " 159,\n",
       " 990,\n",
       " 827,\n",
       " 895,\n",
       " 674,\n",
       " 111,\n",
       " 505,\n",
       " 665,\n",
       " 226,\n",
       " 224,\n",
       " 524,\n",
       " 570,\n",
       " 871,\n",
       " 521,\n",
       " 484,\n",
       " 223,\n",
       " 29,\n",
       " 76,\n",
       " 434,\n",
       " 66,\n",
       " 511,\n",
       " 689,\n",
       " 167,\n",
       " 974,\n",
       " 132,\n",
       " 58,\n",
       " 715,\n",
       " 299,\n",
       " 959,\n",
       " 348,\n",
       " 716,\n",
       " 346,\n",
       " 98,\n",
       " 862,\n",
       " 245]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing value at df_2, 1% with seed 48\n",
    "data_obs_5 = list(range(1, 1001, 1))\n",
    "random.seed(nrp) \n",
    "data_obs_5 = random.sample(data_obs_5, 50)\n",
    "\n",
    "data_obs_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[560, 322, 134, 569, 727, 546, 308, 811, 959, 516]\n",
      "[560, 322, 134, 569, 727, 546, 308, 811, 959, 516, 196, 730, 779, 445, 168, 157, 988, 825, 893, 672, 109, 503, 663, 224, 222, 522, 568, 869, 519, 482, 221, 27, 74, 432, 64, 509, 687, 165, 972, 130, 56, 713, 297, 957, 346, 714, 344, 96, 860, 243]\n"
     ]
    }
   ],
   "source": [
    "data_obs_1 = [x - 2 for x in data_obs_1]\n",
    "data_obs_5 = [x - 2 for x in data_obs_5]\n",
    "\n",
    "print(data_obs_1)\n",
    "print(data_obs_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default     0\n",
      "student     0\n",
      "balance    10\n",
      "income      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# if data_obs_1 found in df_2['balance'] then replace with NaN\n",
    "# if data_obs_5 found in df_2['balance'] then replace with NaN\n",
    "# else, keep the value\n",
    "\n",
    "df_2_missing_1 = df_2.copy()\n",
    "# start from 2\n",
    "for i in data_obs_1:\n",
    "    df_2_missing_1.loc[i, 'balance'] = np.nan\n",
    "\n",
    "df_2_missing_1.to_csv('data/num-1/miss/data-2-missing-1.csv', index=False)\n",
    "\n",
    "print(df_2_missing_1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default     0\n",
      "student     0\n",
      "balance    50\n",
      "income      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_2_missing_5 = df_2.copy()\n",
    "\n",
    "for i in data_obs_5:\n",
    "    df_2_missing_5.loc[i, 'balance'] = np.nan\n",
    "\n",
    "df_2_missing_1.to_csv('data/num-1/miss/data-2-missing-5.csv', index=False)\n",
    "\n",
    "print(df_2_missing_5.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gunakan 3 jenis metode imputasi untuk tiap data missing value yang diperoleh pada langkah 1, yakni:\n",
    "    - Metode 1: Berdasarkan mean/median\n",
    "    - Metode 2 dan 3: bebas (akan mendapatkan poin tambahan apabila menggunakan metode diluar yang pernah dipraktikkan dikelas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pengerjaan df_1_missing_1 dan df_1_missing_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700.0\n",
      "950.0\n",
      "1140.3080808080808\n",
      "\n",
      "700.0\n",
      "955.0\n",
      "1142.8105263157895\n"
     ]
    }
   ],
   "source": [
    "df_1_missing_1 = pd.read_csv('data/num-1/miss/data-1-missing-1.csv')\n",
    "df_1_missing_5 = pd.read_csv('data/num-1/miss/data-1-missing-5.csv')\n",
    "\n",
    "# check mode, median, and mean\n",
    "print(df_1_missing_1['Close'].mode()[0])\n",
    "print(df_1_missing_1['Close'].median())\n",
    "print(df_1_missing_1['Close'].mean())\n",
    "print()\n",
    "print(df_1_missing_5['Close'].mode()[0])\n",
    "print(df_1_missing_5['Close'].median())\n",
    "print(df_1_missing_5['Close'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metode 1 (mean/median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-1-missing-1.csv and data-1-missing-5.csv using mean\n",
    "df_1_missing_1 = pd.read_csv('data/num-1/miss/data-1-missing-1.csv')\n",
    "df_1_missing_5 = pd.read_csv('data/num-1/miss/data-1-missing-5.csv')\n",
    "\n",
    "df_1_missing_1['Close'] = df_1_missing_1['Close'].fillna(df_1_missing_1['Close'].mean())\n",
    "df_1_missing_1.to_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-mean.csv', index=False)\n",
    "\n",
    "df_1_missing_5['Close'] = df_1_missing_5['Close'].fillna(df_1_missing_5['Close'].mean())\n",
    "df_1_missing_5.to_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-1-missing-1.csv and data-1-missing-5.csv using median\n",
    "df_1_missing_1 = pd.read_csv('data/num-1/miss/data-1-missing-1.csv')\n",
    "df_1_missing_5 = pd.read_csv('data/num-1/miss/data-1-missing-5.csv')\n",
    "\n",
    "df_1_missing_1['Close'] = df_1_missing_1['Close'].fillna(df_1_missing_1['Close'].median())\n",
    "df_1_missing_1.to_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-median.csv', index=False)\n",
    "\n",
    "df_1_missing_5['Close'] = df_1_missing_5['Close'].fillna(df_1_missing_5['Close'].median())\n",
    "df_1_missing_5.to_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-median.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metode 2: Modus, Random, Linear Interpolation, KNN, Quadratic Interpolation, Cubic Interpolation, Spline Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-1-missing-1.csv and data-1-missing-5.csv using mode\n",
    "df_1_missing_1 = pd.read_csv('data/num-1/miss/data-1-missing-1.csv')\n",
    "df_1_missing_5 = pd.read_csv('data/num-1/miss/data-1-missing-5.csv')\n",
    "\n",
    "df_1_missing_1['Close'] = df_1_missing_1['Close'].fillna(df_1_missing_1['Close'].mode()[0])\n",
    "df_1_missing_1.to_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-mode.csv', index=False)\n",
    "\n",
    "df_1_missing_5['Close'] = df_1_missing_5['Close'].fillna(df_1_missing_5['Close'].mode()[0])\n",
    "df_1_missing_5.to_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-mode.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-1-missing-1.csv and data-1-missing-5.csv using random\n",
    "df_1_missing_1 = pd.read_csv('data/num-1/miss/data-1-missing-1.csv')\n",
    "df_1_missing_5 = pd.read_csv('data/num-1/miss/data-1-missing-5.csv')\n",
    "\n",
    "df_1_missing_1['Close'] = df_1_missing_1['Close'].fillna(random.choice(df_1_missing_1['Close']))\n",
    "df_1_missing_1.to_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-random.csv', index=False)\n",
    "\n",
    "df_1_missing_5['Close'] = df_1_missing_5['Close'].fillna(random.choice(df_1_missing_5['Close']))\n",
    "df_1_missing_5.to_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-random.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-1-missing-1.csv and data-1-missing-5.csv using linear interpolation\n",
    "df_1_missing_1 = pd.read_csv('data/num-1/miss/data-1-missing-1.csv')\n",
    "df_1_missing_5 = pd.read_csv('data/num-1/miss/data-1-missing-5.csv')\n",
    "\n",
    "df_1_missing_1['Close'] = df_1_missing_1['Close'].interpolate(method='linear')\n",
    "df_1_missing_1.to_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-linear.csv', index=False)\n",
    "\n",
    "df_1_missing_5['Close'] = df_1_missing_5['Close'].interpolate(method='linear')\n",
    "df_1_missing_5.to_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-linear.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-1-missing-1.csv and data-1-missing-5.csv using k-nearest neighbor\n",
    "df_1_missing_1 = pd.read_csv('data/num-1/miss/data-1-missing-1.csv')\n",
    "df_1_missing_5 = pd.read_csv('data/num-1/miss/data-1-missing-5.csv')\n",
    "\n",
    "df_1_missing_1['Close'] = df_1_missing_1['Close'].interpolate(method='linear')\n",
    "df_1_missing_1.to_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-nearest.csv', index=False)\n",
    "\n",
    "df_1_missing_5['Close'] = df_1_missing_5['Close'].interpolate(method='linear')\n",
    "df_1_missing_5.to_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-nearest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-1-missing-1.csv and data-1-missing-5.csv using quadratic interpolation\n",
    "df_1_missing_1 = pd.read_csv('data/num-1/miss/data-1-missing-1.csv')\n",
    "df_1_missing_5 = pd.read_csv('data/num-1/miss/data-1-missing-5.csv')\n",
    "\n",
    "df_1_missing_1['Close'] = df_1_missing_1['Close'].interpolate(method='quadratic')\n",
    "df_1_missing_1.to_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-quadratic.csv', index=False)\n",
    "\n",
    "df_1_missing_5['Close'] = df_1_missing_5['Close'].interpolate(method='quadratic')\n",
    "df_1_missing_5.to_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-quadratic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-1-missing-1.csv and data-1-missing-5.csv using cubic interpolation\n",
    "df_1_missing_1 = pd.read_csv('data/num-1/miss/data-1-missing-1.csv')\n",
    "df_1_missing_5 = pd.read_csv('data/num-1/miss/data-1-missing-5.csv')\n",
    "\n",
    "df_1_missing_1['Close'] = df_1_missing_1['Close'].interpolate(method='cubic')\n",
    "df_1_missing_1.to_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-cubic.csv', index=False)\n",
    "\n",
    "df_1_missing_5['Close'] = df_1_missing_5['Close'].interpolate(method='cubic')\n",
    "df_1_missing_5.to_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-cubic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impuation to data-1-missing-1.csv and data-1-missing-5.csv using slinear interpolation\n",
    "# slinear interpolation only accept integer, so we need to round the value\n",
    "df_1_missing_1 = pd.read_csv('data/num-1/miss/data-1-missing-1.csv')\n",
    "df_1_missing_1['Close'] = df_1_missing_1['Close'].interpolate(method='slinear')\n",
    "df_1_missing_1['Close'] = df_1_missing_1['Close'].round()\n",
    "df_1_missing_1.to_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-slinear.csv', index=False)\n",
    "\n",
    "df_1_missing_5 = pd.read_csv('data/num-1/miss/data-1-missing-5.csv')\n",
    "df_1_missing_5['Close'] = df_1_missing_5['Close'].interpolate(method='slinear')\n",
    "df_1_missing_5['Close'] = df_1_missing_5['Close'].round()\n",
    "df_1_missing_5.to_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-slinear.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pengerjaan df_2_missing_1 dan df_2_missing_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-2-missing-1.csv and data-2-missing-5.csv using mean\n",
    "df_2_missing_1 = pd.read_csv('data/num-1/miss/data-2-missing-1.csv')\n",
    "df_2_missing_5 = pd.read_csv('data/num-1/miss/data-2-missing-5.csv')\n",
    "\n",
    "df_2_missing_1['balance'] = df_2_missing_1['balance'].fillna(df_2_missing_1['balance'].mean())\n",
    "df_2_missing_1.to_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-mean.csv', index=False)\n",
    "\n",
    "df_2_missing_5['balance'] = df_2_missing_5['balance'].fillna(df_2_missing_5['balance'].mean())\n",
    "df_2_missing_5.to_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-2-missing-1.csv and data-2-missing-5.csv using median\n",
    "df_2_missing_1 = pd.read_csv('data/num-1/miss/data-2-missing-1.csv')\n",
    "df_2_missing_5 = pd.read_csv('data/num-1/miss/data-2-missing-5.csv')\n",
    "\n",
    "df_2_missing_1['balance'] = df_2_missing_1['balance'].fillna(df_2_missing_1['balance'].median())\n",
    "df_2_missing_1.to_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-median.csv', index=False)\n",
    "\n",
    "df_2_missing_5['balance'] = df_2_missing_5['balance'].fillna(df_2_missing_5['balance'].median())\n",
    "df_2_missing_5.to_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-median.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-2-missing-1.csv and data-2-missing-5.csv using mode\n",
    "df_2_missing_1 = pd.read_csv('data/num-1/miss/data-2-missing-1.csv')\n",
    "df_2_missing_5 = pd.read_csv('data/num-1/miss/data-2-missing-5.csv')\n",
    "\n",
    "df_2_missing_1['balance'] = df_2_missing_1['balance'].fillna(df_2_missing_1['balance'].mode()[0])\n",
    "df_2_missing_1.to_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-mode.csv', index=False)\n",
    "\n",
    "df_2_missing_5['balance'] = df_2_missing_5['balance'].fillna(df_2_missing_5['balance'].mode()[0])\n",
    "df_2_missing_5.to_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-mode.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-2-missing-1.csv and data-2-missing-5.csv using random\n",
    "df_2_missing_1 = pd.read_csv('data/num-1/miss/data-2-missing-1.csv')\n",
    "df_2_missing_5 = pd.read_csv('data/num-1/miss/data-2-missing-5.csv')\n",
    "\n",
    "df_2_missing_1['balance'] = df_2_missing_1['balance'].fillna(random.choice(df_2_missing_1['balance']))\n",
    "df_2_missing_1.to_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-random.csv', index=False)\n",
    "\n",
    "df_2_missing_5['balance'] = df_2_missing_5['balance'].fillna(random.choice(df_2_missing_5['balance']))\n",
    "df_2_missing_5.to_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-random.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-2-missing-1.csv and data-2-missing-5.csv using linear interpolation\n",
    "df_2_missing_1 = pd.read_csv('data/num-1/miss/data-2-missing-1.csv')\n",
    "df_2_missing_5 = pd.read_csv('data/num-1/miss/data-2-missing-5.csv')\n",
    "\n",
    "df_2_missing_1['balance'] = df_2_missing_1['balance'].interpolate(method='linear')\n",
    "df_2_missing_1.to_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-linear.csv', index=False)\n",
    "\n",
    "df_2_missing_5['balance'] = df_2_missing_5['balance'].interpolate(method='linear')\n",
    "df_2_missing_5.to_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-linear.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-2-missing-1.csv and data-2-missing-5.csv using k-nearest neighbor\n",
    "df_2_missing_1 = pd.read_csv('data/num-1/miss/data-2-missing-1.csv')\n",
    "df_2_missing_5 = pd.read_csv('data/num-1/miss/data-2-missing-5.csv')\n",
    "\n",
    "df_2_missing_1['balance'] = df_2_missing_1['balance'].interpolate(method='linear')\n",
    "df_2_missing_1.to_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-nearest.csv', index=False)\n",
    "\n",
    "df_2_missing_5['balance'] = df_2_missing_5['balance'].interpolate(method='linear')\n",
    "df_2_missing_5.to_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-nearest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-2-missing-1.csv and data-2-missing-5.csv using quadratic interpolation\n",
    "df_2_missing_1 = pd.read_csv('data/num-1/miss/data-2-missing-1.csv')\n",
    "df_2_missing_5 = pd.read_csv('data/num-1/miss/data-2-missing-5.csv')\n",
    "\n",
    "df_2_missing_1['balance'] = df_2_missing_1['balance'].interpolate(method='quadratic')\n",
    "df_2_missing_1.to_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-quadratic.csv', index=False)\n",
    "\n",
    "df_2_missing_5['balance'] = df_2_missing_5['balance'].interpolate(method='quadratic')\n",
    "df_2_missing_5.to_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-quadratic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-2-missing-1.csv and data-2-missing-5.csv using cubic interpolation\n",
    "df_2_missing_1 = pd.read_csv('data/num-1/miss/data-2-missing-1.csv')\n",
    "df_2_missing_5 = pd.read_csv('data/num-1/miss/data-2-missing-5.csv')\n",
    "\n",
    "df_2_missing_1['balance'] = df_2_missing_1['balance'].interpolate(method='cubic')\n",
    "df_2_missing_1.to_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-cubic.csv', index=False)\n",
    "\n",
    "df_2_missing_5['balance'] = df_2_missing_5['balance'].interpolate(method='cubic')\n",
    "df_2_missing_5.to_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-cubic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation to data-2-missing-1.csv and data-2-missing-5.csv using slinear interpolation\n",
    "# slinear interpolation only accept integer, so we need to round the value\n",
    "df_2_missing_1 = pd.read_csv('data/num-1/miss/data-2-missing-1.csv')\n",
    "df_2_missing_1['balance'] = df_2_missing_1['balance'].interpolate(method='slinear')\n",
    "df_2_missing_1['balance'] = df_2_missing_1['balance'].round()\n",
    "df_2_missing_1.to_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-slinear.csv', index=False)\n",
    "\n",
    "df_2_missing_5 = pd.read_csv('data/num-1/miss/data-2-missing-5.csv')\n",
    "df_2_missing_5['balance'] = df_2_missing_5['balance'].interpolate(method='slinear')\n",
    "df_2_missing_5['balance'] = df_2_missing_5['balance'].round()\n",
    "df_2_missing_5.to_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-slinear.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Hitung nilai MAPE dengan membandingkan data asli dan data hasil imputasi, yakni menggunakan persamaan berikut:\n",
    "\n",
    "$$MAPE = \\frac{100}{M} \\sum\\limits_{i=1}^{M} \\left|\\frac{Y_i - \\hat{Y}_i}{Y_i}\\right|$$\n",
    "\n",
    "dengan:\n",
    "- Yi merupakan data aktual ke-i\n",
    "- Yi' merupakan hasil imputasi untuk data ke-i\n",
    "- M merupakan banyaknya pengamatan data missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Percentage Error (MAPE).\n",
    "\n",
    "    Parameters:\n",
    "    Y (list or numpy array): Actual values.\n",
    "    Y_hat (list or numpy array): Predicted values.\n",
    "\n",
    "    Returns:\n",
    "    float: MAPE value.\n",
    "    \"\"\"\n",
    "    if len(Y) != len(Y_hat):\n",
    "        raise ValueError(\"Lengths of Y and Y_hat must be the same.\")\n",
    "\n",
    "    M = len(Y)\n",
    "    mape_sum = 0\n",
    "\n",
    "    for i in range(M):\n",
    "        mape_sum += abs((Y[i] - Y_hat[i]) / Y[i])\n",
    "\n",
    "    mape = (mape_sum / M) * 100\n",
    "    return mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE for data-1-missing-1.csv\n",
      "mean:  0.2639131409894346\n",
      "median:  0.22097069089801313\n",
      "mode:  0.33229613828879934\n",
      "random:  0.22352944841102437\n",
      "linear:  0.012207596718810748\n",
      "nearest: 0.012207596718810748\n",
      "quadratic: 0.017234627980690163\n",
      "cubic: 0.01815532264000183\n",
      "slinear: 0.012207596718810748\n"
     ]
    }
   ],
   "source": [
    "# count MAPE based on \\frac{100}{M} \\sum\\limits_{i=1}^{M} \\left|\\frac{Y_i - \\hat{Y}_i}{Y_i}\\right|$$ \n",
    "# where M is the number of missing data and Y_i is the ith actual value and \\hat{Y}_i is the ith predicted value.\n",
    "\n",
    "# MAPE for data-1-missing-1.csv\n",
    "df_1 = pd.read_csv('data/num-1/raw/data-1.csv')\n",
    "df_1_missing_1_mean = pd.read_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-mean.csv')\n",
    "df_1_missing_1_median = pd.read_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-median.csv')\n",
    "df_1_missing_1_mode = pd.read_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-mode.csv')\n",
    "df_1_missing_1_random = pd.read_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-random.csv')\n",
    "df_1_missing_1_linear = pd.read_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-linear.csv')\n",
    "df_1_missing_1_nearest = pd.read_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-nearest.csv')\n",
    "df_1_missing_1_quadratic = pd.read_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-quadratic.csv')\n",
    "df_1_missing_1_cubic = pd.read_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-cubic.csv')\n",
    "df_1_missing_1_slinear = pd.read_csv('data/num-2/data-1-imputation/miss-1/data-1-missing-1-imputation-slinear.csv')\n",
    "\n",
    "# calculation\n",
    "# mean\n",
    "mape_mean = calculate_mape(df_1['Close'], df_1_missing_1_mean['Close'])\n",
    "\n",
    "# median\n",
    "mape_median = calculate_mape(df_1['Close'], df_1_missing_1_median['Close'])\n",
    "\n",
    "# mode\n",
    "mape_mode = calculate_mape(df_1['Close'], df_1_missing_1_mode['Close'])\n",
    "\n",
    "# random\n",
    "mape_random = calculate_mape(df_1['Close'], df_1_missing_1_random['Close'])\n",
    "\n",
    "# linear\n",
    "mape_linear = calculate_mape(df_1['Close'], df_1_missing_1_linear['Close'])\n",
    "\n",
    "# nearest\n",
    "mape_nearest = calculate_mape(df_1['Close'], df_1_missing_1_nearest['Close'])\n",
    "\n",
    "# quadratic\n",
    "mape_quadratic = calculate_mape(df_1['Close'], df_1_missing_1_quadratic['Close'])\n",
    "\n",
    "# cubic\n",
    "mape_cubic = calculate_mape(df_1['Close'], df_1_missing_1_cubic['Close'])\n",
    "\n",
    "\n",
    "# slinear\n",
    "mape_slinear = calculate_mape(df_1['Close'], df_1_missing_1_slinear['Close'])\n",
    "\n",
    "print('MAPE for data-1-missing-1.csv')\n",
    "print('mean: ', mape_mean)\n",
    "print('median: ', mape_median)\n",
    "print('mode: ', mape_mode)\n",
    "print('random: ', mape_random)\n",
    "print('linear: ', mape_linear)\n",
    "print('nearest:', mape_nearest)\n",
    "print('quadratic:', mape_quadratic)\n",
    "print('cubic:', mape_cubic)\n",
    "print('slinear:', mape_slinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE for data-1-missing-1.csv\n",
      "mean:  1.8076776392392202\n",
      "median:  1.3212813620846267\n",
      "mode:  1.4059484557399713\n",
      "random:  1.209315931885786\n",
      "linear:  0.07447314767751355\n",
      "nearest: 0.07447314767751355\n",
      "quadratic: 0.09204284131528855\n",
      "cubic: 0.09604791141554275\n",
      "slinear: 0.07441810719776504\n"
     ]
    }
   ],
   "source": [
    "# count MAPE based on \\frac{100}{M} \\sum\\limits_{i=1}^{M} \\left|\\frac{Y_i - \\hat{Y}_i}{Y_i}\\right|$$ \n",
    "# where M is the number of missing data and Y_i is the ith actual value and \\hat{Y}_i is the ith predicted value.\n",
    "\n",
    "# MAPE for data-1-missing-1.csv\n",
    "df_1 = pd.read_csv('data/num-1/raw/data-1.csv')\n",
    "df_1_missing_5_mean = pd.read_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-mean.csv')\n",
    "df_1_missing_5_median = pd.read_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-median.csv')\n",
    "df_1_missing_5_mode = pd.read_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-mode.csv')\n",
    "df_1_missing_5_random = pd.read_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-random.csv')\n",
    "df_1_missing_5_linear = pd.read_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-linear.csv')\n",
    "df_1_missing_5_nearest = pd.read_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-nearest.csv')\n",
    "df_1_missing_5_quadratic = pd.read_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-quadratic.csv')\n",
    "df_1_missing_5_cubic = pd.read_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-cubic.csv')\n",
    "df_1_missing_5_slinear = pd.read_csv('data/num-2/data-1-imputation/miss-5/data-1-missing-5-imputation-slinear.csv')\n",
    "\n",
    "# calculation\n",
    "# mean\n",
    "mape_mean = calculate_mape(df_1['Close'], df_1_missing_5_mean['Close'])\n",
    "\n",
    "# median\n",
    "mape_median = calculate_mape(df_1['Close'], df_1_missing_5_median['Close'])\n",
    "\n",
    "# mode\n",
    "mape_mode = calculate_mape(df_1['Close'], df_1_missing_5_mode['Close'])\n",
    "\n",
    "# random\n",
    "mape_random = calculate_mape(df_1['Close'], df_1_missing_5_random['Close'])\n",
    "\n",
    "# linear\n",
    "mape_linear = calculate_mape(df_1['Close'], df_1_missing_5_linear['Close'])\n",
    "\n",
    "# nearest\n",
    "mape_nearest = calculate_mape(df_1['Close'], df_1_missing_5_nearest['Close'])\n",
    "\n",
    "# quadratic\n",
    "mape_quadratic = calculate_mape(df_1['Close'], df_1_missing_5_quadratic['Close'])\n",
    "\n",
    "# cubic\n",
    "mape_cubic = calculate_mape(df_1['Close'], df_1_missing_5_cubic['Close'])\n",
    "\n",
    "# slinear\n",
    "mape_slinear = calculate_mape(df_1['Close'], df_1_missing_5_slinear['Close'])\n",
    "\n",
    "print('MAPE for data-1-missing-5.csv')\n",
    "print('mean: ', mape_mean)\n",
    "print('median: ', mape_median)\n",
    "print('mode: ', mape_mode)\n",
    "print('random: ', mape_random)\n",
    "print('linear: ', mape_linear)\n",
    "print('nearest:', mape_nearest)\n",
    "print('quadratic:', mape_quadratic)\n",
    "print('cubic:', mape_cubic)\n",
    "print('slinear:', mape_slinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE for data-1-missing-1.csv\n",
      "mean:  0.6578698341612838\n",
      "median:  0.6199055657397791\n",
      "mode:  0.9977826975153709\n",
      "random:  0.4066316697202847\n",
      "linear:  0.7804153238488921\n",
      "nearest: 0.7804153238488921\n",
      "quadratic: 1.064723364840966\n",
      "cubic: 1.15541961950536\n",
      "slinear: 0.8747004820412555\n"
     ]
    }
   ],
   "source": [
    "# count MAPE based on \\frac{100}{M} \\sum\\limits_{i=1}^{M} \\left|\\frac{Y_i - \\hat{Y}_i}{Y_i}\\right|$$ \n",
    "# where M is the number of missing data and Y_i is the ith actual value and \\hat{Y}_i is the ith predicted value.\n",
    "\n",
    "# MAPE for data-1-missing-1.csv\n",
    "df_2 = pd.read_csv('data/num-1/raw/data-2.csv')\n",
    "df_2_missing_1_mean = pd.read_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-mean.csv')\n",
    "df_2_missing_1_median = pd.read_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-median.csv')\n",
    "df_2_missing_1_mode = pd.read_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-mode.csv')\n",
    "df_2_missing_1_random = pd.read_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-random.csv')\n",
    "df_2_missing_1_linear = pd.read_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-linear.csv')\n",
    "df_2_missing_1_nearest = pd.read_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-nearest.csv')\n",
    "df_2_missing_1_quadratic = pd.read_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-quadratic.csv')\n",
    "df_2_missing_1_cubic = pd.read_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-cubic.csv')\n",
    "df_2_missing_1_slinear = pd.read_csv('data/num-2/data-2-imputation/miss-1/data-2-missing-1-imputation-slinear.csv')\n",
    "\n",
    "# calculation\n",
    "# mean\n",
    "mape_mean = calculate_mape(df_2['balance'], df_2_missing_1_mean['balance'])\n",
    "\n",
    "# median\n",
    "mape_median = calculate_mape(df_2['balance'], df_2_missing_1_median['balance'])\n",
    "\n",
    "# mode\n",
    "mape_mode = calculate_mape(df_2['balance'], df_2_missing_1_mode['balance'])\n",
    "\n",
    "# random\n",
    "mape_random = calculate_mape(df_2['balance'], df_2_missing_1_random['balance'])\n",
    "\n",
    "# linear\n",
    "mape_linear = calculate_mape(df_2['balance'], df_2_missing_1_linear['balance'])\n",
    "\n",
    "# nearest\n",
    "mape_nearest = calculate_mape(df_2['balance'], df_2_missing_1_nearest['balance'])\n",
    "\n",
    "# quadratic\n",
    "mape_quadratic = calculate_mape(df_2['balance'], df_2_missing_1_quadratic['balance'])\n",
    "\n",
    "# cubic\n",
    "mape_cubic = calculate_mape(df_2['balance'], df_2_missing_1_cubic['balance'])\n",
    "\n",
    "# slinear\n",
    "mape_slinear = calculate_mape(df_2['balance'], df_2_missing_1_slinear['balance'])\n",
    "\n",
    "print('MAPE for data-2-missing-5.csv')\n",
    "print('mean: ', mape_mean)\n",
    "print('median: ', mape_median)\n",
    "print('mode: ', mape_mode)\n",
    "print('random: ', mape_random)\n",
    "print('linear: ', mape_linear)\n",
    "print('nearest:', mape_nearest)\n",
    "print('quadratic:', mape_quadratic)\n",
    "print('cubic:', mape_cubic)\n",
    "print('slinear:', mape_slinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE for data-2-missing-5.csv\n",
      "mean:  0.6578698341612838\n",
      "median:  0.6199055657397791\n",
      "mode:  0.9977826975153709\n",
      "random:  0.40661547378983803\n",
      "linear:  0.7804153238488921\n",
      "nearest: 0.7804153238488921\n",
      "quadratic: 1.064723364840966\n",
      "cubic: 1.15541961950536\n",
      "slinear: 0.8747004820412555\n"
     ]
    }
   ],
   "source": [
    "# count MAPE based on \\frac{100}{M} \\sum\\limits_{i=1}^{M} \\left|\\frac{Y_i - \\hat{Y}_i}{Y_i}\\right|$$ \n",
    "# where M is the number of missing data and Y_i is the ith actual value and \\hat{Y}_i is the ith predicted value.\n",
    "\n",
    "# MAPE for data-1-missing-1.csv\n",
    "df_2 = pd.read_csv('data/num-1/raw/data-2.csv')\n",
    "df_2_missing_5_mean = pd.read_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-mean.csv')\n",
    "df_2_missing_5_median = pd.read_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-median.csv')\n",
    "df_2_missing_5_mode = pd.read_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-mode.csv')\n",
    "df_2_missing_5_random = pd.read_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-random.csv')\n",
    "df_2_missing_5_linear = pd.read_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-linear.csv')\n",
    "df_2_missing_5_nearest = pd.read_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-nearest.csv')\n",
    "df_2_missing_5_quadratic = pd.read_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-quadratic.csv')\n",
    "df_2_missing_5_cubic = pd.read_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-cubic.csv')\n",
    "df_2_missing_5_slinear = pd.read_csv('data/num-2/data-2-imputation/miss-5/data-2-missing-5-imputation-slinear.csv')\n",
    "\n",
    "# calculation\n",
    "# mean\n",
    "mape_mean = calculate_mape(df_2['balance'], df_2_missing_5_mean['balance'])\n",
    "\n",
    "# median\n",
    "mape_median = calculate_mape(df_2['balance'], df_2_missing_5_median['balance'])\n",
    "\n",
    "# mode\n",
    "mape_mode = calculate_mape(df_2['balance'], df_2_missing_5_mode['balance'])\n",
    "\n",
    "# random\n",
    "mape_random = calculate_mape(df_2['balance'], df_2_missing_5_random['balance'])\n",
    "\n",
    "# linear\n",
    "mape_linear = calculate_mape(df_2['balance'], df_2_missing_5_linear['balance'])\n",
    "\n",
    "# nearest\n",
    "mape_nearest = calculate_mape(df_2['balance'], df_2_missing_5_nearest['balance'])\n",
    "\n",
    "# quadratic\n",
    "mape_quadratic = calculate_mape(df_2['balance'], df_2_missing_5_quadratic['balance'])\n",
    "\n",
    "# cubic\n",
    "mape_cubic = calculate_mape(df_2['balance'], df_2_missing_5_cubic['balance'])\n",
    "\n",
    "# slinear\n",
    "mape_slinear = calculate_mape(df_2['balance'], df_2_missing_5_slinear['balance'])\n",
    "\n",
    "print('MAPE for data-2-missing-5.csv')\n",
    "print('mean: ', mape_mean)\n",
    "print('median: ', mape_median)\n",
    "print('mode: ', mape_mode)\n",
    "print('random: ', mape_random)\n",
    "print('linear: ', mape_linear)\n",
    "print('nearest:', mape_nearest)\n",
    "print('quadratic:', mape_quadratic)\n",
    "print('cubic:', mape_cubic)\n",
    "print('slinear:', mape_slinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduksi Dimensi\n",
    "\n",
    "Data yang tersedia merupakan data hasil survey sosial ekonomi di United States.\n",
    "Lakukan reduksi dimensi menggunakan PCA pada data sesuai dengan akhiran NRP anda (sertakan langkahnya secara lengkap dan tuliskan juga persamaan PC nya)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>PctImmigRecent</th>\n",
       "      <th>PctImmigRec5</th>\n",
       "      <th>PctImmigRec8</th>\n",
       "      <th>PctImmigRec10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1994 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      population  householdsize  PctImmigRecent  PctImmigRec5  PctImmigRec8  \\\n",
       "0           0.19           0.33            0.24          0.27          0.37   \n",
       "1           0.00           0.16            0.52          0.62          0.64   \n",
       "2           0.00           0.42            0.07          0.06          0.15   \n",
       "3           0.04           0.77            0.11          0.20          0.30   \n",
       "4           0.01           0.55            0.03          0.07          0.20   \n",
       "...          ...            ...             ...           ...           ...   \n",
       "1989        0.01           0.40            0.42          0.41          0.42   \n",
       "1990        0.05           0.96            0.56          0.62          0.63   \n",
       "1991        0.16           0.37            0.12          0.17          0.24   \n",
       "1992        0.08           0.51            0.40          0.46          0.48   \n",
       "1993        0.20           0.78            0.43          0.52          0.58   \n",
       "\n",
       "      PctImmigRec10  \n",
       "0              0.39  \n",
       "1              0.63  \n",
       "2              0.19  \n",
       "3              0.31  \n",
       "4              0.27  \n",
       "...             ...  \n",
       "1989           0.47  \n",
       "1990           0.67  \n",
       "1991           0.26  \n",
       "1992           0.49  \n",
       "1993           0.65  \n",
       "\n",
       "[1994 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.116208</td>\n",
       "      <td>-0.137581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.437517</td>\n",
       "      <td>-0.304325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.523724</td>\n",
       "      <td>-0.024664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.286717</td>\n",
       "      <td>0.319194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.472648</td>\n",
       "      <td>0.111739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>0.101774</td>\n",
       "      <td>-0.065256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>0.499937</td>\n",
       "      <td>0.481456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>-0.355087</td>\n",
       "      <td>-0.091573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>0.164561</td>\n",
       "      <td>0.038902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>0.350746</td>\n",
       "      <td>0.301576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1994 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PC1       PC2\n",
       "0    -0.116208 -0.137581\n",
       "1     0.437517 -0.304325\n",
       "2    -0.523724 -0.024664\n",
       "3    -0.286717  0.319194\n",
       "4    -0.472648  0.111739\n",
       "...        ...       ...\n",
       "1989  0.101774 -0.065256\n",
       "1990  0.499937  0.481456\n",
       "1991 -0.355087 -0.091573\n",
       "1992  0.164561  0.038902\n",
       "1993  0.350746  0.301576\n",
       "\n",
       "[1994 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lakukan reduksi dimensi menggunakan PCA pada data sesuai dengan akhiran NRP anda (sertakan langkahnya secara lengkap dan tuliskan juga persamaan PC nya).\n",
    "# PCA\n",
    "# 1. hitung mean dari setiap kolom\n",
    "mean = df.mean(axis=0)\n",
    "# 2. kurangi setiap nilai dengan mean\n",
    "df = df - mean\n",
    "# 3. hitung covariance matrix\n",
    "covariance_matrix = np.cov(df.T)\n",
    "# 4. hitung eigen value dan eigen vector dari covariance matrix\n",
    "eigen_value, eigen_vector = np.linalg.eig(covariance_matrix)\n",
    "# 5. hitung nilai PC\n",
    "PC = df.dot(eigen_vector[:, :nrp]) # hanya 48 komponen utama\n",
    "# 6. hitung nilai PC1 dan PC2\n",
    "PC1 = PC[0]\n",
    "PC2 = PC[1]\n",
    "\n",
    "mixed = {\n",
    "    'PC1': PC1,\n",
    "    'PC2': PC2\n",
    "}\n",
    "\n",
    "df_mixed = pd.DataFrame(mixed, columns=['PC1', 'PC2'])\n",
    "df_mixed.to_csv('data/PC.csv', index=False)\n",
    "\n",
    "df_mixed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
